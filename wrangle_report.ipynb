{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapport: wragle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecte des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette première étape du processus du traitement et d'analyse des données, nous avons commencé par télécharger et lire la première base de donnée twitter_archive qui été sous format csv et  que nous avons par la suite stocker dans un DataFrame. Ensuite nous avons télécharger de façon programatique la base de données sur les prédictions des races que nous avons stocker aussi dans un DataFrame. Enfin nous avons télécharger le fichier Json que nous avons lu ligne par ligne pour extraire les données supplementaires, les données qui concernaient le nombre de retweet et le nombre de favoris de chaque chien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce qui est de cette étape du processus, nous avons utilisé l'évaluation visuelle et programatique pour detecter les problèmes de qualité et des problèmes d'ordre. Nous avons utilisé l'ensemble des trois bases de données pour détecter ces problèmes, que nous avons par la suite proposé des solutions. Concernant les données twitter_archives nous avons par exemple identifier les problèmes de qualité comme les noms qui étaient mal extrait, ou encore la répétition d'une ou plusieur url dans la colonne expanded_url; et comme problème de structure nous avons par exemple remarquer qu'il y'avait deux observations en surplus par rapport aux données additionnelle, que quatre variables se retrouvaient dans la même colonne. Pour ce qui est des données additionnelle nous avons rencontré que les erreurs de type. Et enfin dans la dernière base de données nous avons vu que la dimension de la base n'était pas la même avec celle de la première."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce qui est du nettoyage des données nous avons commencé par regler les problèmes de structure et ensuite les problèmes de qualité. Nous avons donc commencer par retirer les deux observations qui manquait dans les données additionnelle de la base twitter_archive, ensuite rétirer les observations qui avaient plus d'une race, nous avons terminé de résoudre les problèmes structurelles par fusionner les trois bases de données. La résolution des problèmes de qualité à commencer par le traitement des noms mal extraits ou mal écrit, et à fini par la résolution des problèmes de type, en passant par le remplacement des None en NaN ou encore la resolution du problème des url qui se repetaient. Tout au long du proccessus de nettoyage nous avons défini, codé et testé nos codes. Dans le test de nos codes nous avons principalement utulisé les méthode info(), value_counts() ou shape."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
